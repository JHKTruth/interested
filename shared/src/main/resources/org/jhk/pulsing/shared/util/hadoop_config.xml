<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <hadoop>
    <!--
    /etl : Data in various stages of being processed by an ETL with input for the landing zone and have directories for each stage of the process.
     input - landing zone
     processing - intermediate stage
     output - final result
     bad - rejected
    /etc/<group>/<application>/<process>/{input,processing,output,bad}
    
    /tmp : Temporary data generated by tools or shared between users
    
    /data : Data sets that have been processed and are shared across the organization. Since /data serves as the location for shared processed data sets, it 
     will contain subdirectories for each data set. /data/friendship/..., /data/pulses
    
    /app : Includes everything required for Hadoop applications to run, except data. This includes JAR files, Oozie workflow definitions, Hive HQL files, and more.
      This directory should have a subdirectory for each group and application, similar to the structure used in /etl. For a given application (say Oozie), you would 
      need a directory for each version of the artifacts you decide to store in HFDS, possibly tagging, via a sym-link in HFDS, the latest artifact as latest and
      the currently used one as current. The directories containing the binary artifacts would be present under these versioned directories. This will look similar to:
      /app/<group>/<application>/<version>/<artifact directory>/<artifact>
    
    /metadata : Some extra metadata apart from Hive metatstore.
     -->
    <property>
      <name>training.data.workspace</name>
      <value>/training/data/</value>
    </property>
    
    <property>
      <name>spark.master.workspace</name>
      <value>/data/sparkmaster/</value>
    </property>
    
    <property>
      <name>hive.master.workspace</name>
      <value>/data/hivemaster/</value>
    </property>
    
    <property>
      <name>pail.master.workspace</name>
      <value>/data/pailmaster/</value>
    </property>
    
    <property>
      <name>spark.newdata.workspace</name>
      <value>/data/sparknewdata/</value>
    </property>
    
    <property>
      <name>hive.newdata.workspace</name>
      <value>/data/hivenewdata/</value>
    </property>
    
    <property>
      <name>pail.newdata.workspace</name>
      <value>/data/pailnewdata/</value>
    </property>
    
    <property>
      <name>tempdata.workspace</name>
      <value>/tmp/</value>
    </property>
    
    <property>
      <name>url.port</name>
      <value>hdfs://pulsing.jhk.org</value>
    </property>
  </hadoop>
</configuration>
