<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  
  <!-- 
  Hadoop control scripts (but not the daemons) rely on SSH to perform cluster-wide operations. For example, 
  there is a script for stopping and starting all the daemons in the cluster. Control scripts are optional--cluster-wide 
  operations can be performed by other mechanisms, too such as a distributed shell or dedicated Hadoop management applications.
  
  SSH needs to be setup to allow passwordless login for the hdfs and yarn users from machines in the cluster.
  
  ssh-keygen -t rsa -f ~/.ssh/id_rsa
  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
   -->
  
  <!--
  The default filesystem. THe URI defines the hostname and port that the namenode's RPC server runs on.
  
  The default port is 8020.
   -->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://0.0.0.0</value>
  </property>
  
  <!-- 
  You may configure access control lists in the hadoop-policy.xml configuration file to control which 
  users and groups have permission to connect to each Hadoop service. Services are defined at the protocol
  level, so there are ones for MapReduce job submission, namenode communication, and so on.
  
  kinit
   -->
  <!-- 
  <property>
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
  </property>
   -->
   
</configuration>
